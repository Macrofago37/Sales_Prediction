---
title: "Sales_prediction"
author: "Luiz Felipe Martucci"
date: "4/4/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Libraries_and_FNs, include=FALSE}

Packs <- c("tidyverse", "gridExtra", "randomForest", "caret", "parallel", "doParallel", "purrr", "ranger")
(function(x){
  sapply(x, function(x) if(!x %in% installed.packages()){
    install.packages(x, dependencies = T)
  })
  sapply(x, library, character.only=T)
})(Packs)


# function to calculate RMSE
Pred_RMSE <- function(Model, Data){
  
  #Extract variable y from model
  Args_Formula <- Model[sapply(Model, purrr::is_formula)]$terms[1]
  name_y <- sub("~.*","", Args_Formula )[2]
  y <- Data[, name_y]
  
  
  #Predict 
  Pred <- predict(Model, Data)
  
  sqrt(mean((y - Pred)^2))

}

#Unregister parallel 
Un_parallel <- function() {
 env <- foreach:::.foreachGlobals
 
 rm(list=ls(name=env), pos=env)
}

```

This Hackaton challenge by Analytics Vidhya is about Sales Prediction on BigMart Outlets. The goal is to create a model to predict the sales of each product at a particular outlet.

```{r Loading_dataset}

#Train dataset
BigMart_train <- read.csv("Train_dataset.csv", header = TRUE, na.strings = c("", "NA"))

#Test dataset (doesn't have the target variable). #The target variable (y) is the Item_Outlet_Sales. 
BigMart_test <- read.csv("Test_dataset.csv", header = TRUE, na.strings = c("", "NA"))

#Combining both datasets
BigMart <- BigMart_test %>% mutate(Item_Outlet_Sales= NA) %>% 
  rbind(BigMart_train)


```

```{r Correct_variable_types}
# The column Item_Fat_Content contains overlaying labels for Low_fat, such as LF, Low fat, and Low fat. The same thing happens to Regular, which has Reg and Regular. Therefore this needs to be corrected

#BigMart$Item_Fat_Content %>% unique()

BigMart <- BigMart %>% mutate(
  Item_Fat_Content= ifelse(Item_Fat_Content %in% c("LF", "low fat", "Low fat"),
                           "Low Fat", "Regular"))


#Correct variable types
BigMart<-  BigMart %>% mutate(across(where(is.character), as.factor),
                                Outlet_Establishment_Year= factor(
                                  Outlet_Establishment_Year))


# Updating train and test set
BigMart_train <- BigMart %>% slice(nrow(BigMart_test)+1:nrow(BigMart_train))
BigMart_test <- BigMart %>% slice(1:nrow(BigMart_test))




#### Categorical Predictors vs. the predicted variable #####



```

```{r EDA_Distribution_continuous_data}

# The sale data is right-skewed.
BigMart_train %>% ggplot(aes(Item_Outlet_Sales))+
  geom_histogram(binwidth = 80,
                 fill= "#6669A9")+
  labs(y= "Frequency",
       x= "Sales")


# Distribution of the continuous data.
BigMart %>% select(which(sapply(., class)=="numeric"), -Item_Outlet_Sales) %>% 
  pivot_longer(cols=everything(),
               names_to = "Col") %>% 
  ggplot(aes(value)) +
  geom_histogram(bins = 100,
                 fill="#6669A9")+
  facet_grid(~Col, scales = "free")




```

```{r EDA_Distribution_categorical_data}

(function(){
  BigMart_Factor <- BigMart %>% select(
    which(sapply(., class)=="factor"), -Item_Identifier) %>% 
    select(Item_Type, everything()) #Change order
  
  Columns <- colnames(BigMart_Factor)
  

  Graphs <-  apply(BigMart_Factor,2, function(x){
    BigMart_Factor %>% 
      ggplot()+
      geom_bar(aes(x), fill="#6669A9")+
      ylab("Frequency")+
      theme(axis.text.x= element_text(angle=45, vjust = .5, hjust = 0.5),
            axis.title.x = element_blank())
  })
 
  gridExtra::grid.arrange(grobs=Graphs,
                          layout_matrix= rbind(c(1,1,1),
                                               c(2,3,4),
                                               c(5,6,7)))
})()
 # As we can see, supermarket_type_1 has more sales than other outlet types. Furthermore, and regular food sells more than low-fat food. 


```

```{r EDA_num_predictors_vs_y}

(function(){
  #Cria os gráficos que são númericos
  Graphs <- rapply(BigMart_train, function(x){
    BigMart_train  %>% ggplot()+
      geom_point(aes(x, Item_Outlet_Sales), colour="#874D5E", alpha=.4) +
      labs(y="Item Outlet Sales")
    
  }, classes = "numeric", how="list") %>% 
    discard(is.null)
  
  #Renomeia o eixo x
  Nomes <- str_replace(names(Graphs), "_", " ")
  Corrected_graphs <- map2(Graphs, Nomes, function(Graphs, Nomes){
    Graphs + labs(x=Nomes)
  })
  Corrected_graphs["Item_Outlet_Sales"] <- NULL
  
  #Plota os gráficos
  gridExtra::grid.arrange(grobs=Corrected_graphs)
  
})()
# It is strange to have items with zero visibility.
# There is a clear pattern of 4 categories of items in Items MRP.



```

```{r EDA_cat_predictors_vs_y}


(function(){
  #Arruma a sequência e cria os gráficos que são categóricos 
  Data <- BigMart_train %>% select(Item_Type, where(is.factor), -Item_Identifier, Item_Outlet_Sales)
  Graphs <- lapply(Data, function(x){
    Data %>% ggplot()+
      geom_violin(aes(x, Item_Outlet_Sales), fill="#874D5E") +
      labs(y="Item Outlet Sales")+
      theme(axis.text.x= element_text(angle=45, vjust = .5, hjust = 0.5))
    
  })
  
  #Renomeia o eixo x
  Graphs["Item_Outlet_Sales"] <- NULL
  Nomes <- str_replace(names(Graphs), "_", " ")
  Corrected_graphs <- map2(Graphs, Nomes, function(Graphs, Nomes){
    Graphs + labs(x=Nomes)
  })
  
  gridExtra::grid.arrange(grobs=Corrected_graphs,
                          layout_matrix= rbind(c(1,1),
                                               c(2,3),
                                               c(4,5),
                                               c(6,7)))

})()
# The sales patterns on the outlet OUTO10 and OUT019 are similar and different from everyone else. 
# A similar thing happens between small outlets and NA outlets. Therefore, the NA data probably is about small outlets and will be defined as so.




```

```{r Impute_NA_create_variables_pt1}


BigMart <- (function(){
  # Classify Items_Type into perishable and non-perishable.
  Perishable <- c("Dairy",
                  "Fruits and Vegetables", 
                  "Seafood",
                  "Meat",
                  "Breakfast",
                  "Starchy Foods",
                  "Breads")
  
  Non_perishable <- c("Baking Goods",
                      "Frozen Foods",
                      "Health and Hygiene",
                      "Hard Drinks",
                      "Soft Drinks",
                      "Household",
                      "Canned")
  
  # Define cutoff points for Item_MRP clusters.
  Cutoff_Item_MRP <- kmeans(BigMart$Item_MRP, 4)$centers %>% sort()
  
  BigMart %>% mutate(Perishable_State=case_when(
    Item_Type %in% Perishable ~ "Perishable",
    Item_Type %in% Non_perishable ~ "Non-Perishable",
    TRUE ~ "Other"),
    #NA Outlet_Size to Small
    Outlet_Size= ifelse(is.na(Outlet_Size),
                        "Small", paste(Outlet_Size)),
    # Splitting the first two letters of Item_Identifier to use as a new product identifier
    Cat_Item_Identifier= substr(Item_Identifier, 1, 2),
    # Change non-edible items from Low-fat to non-edible.
    Item_Fat_Content= ifelse(Cat_Item_Identifier=="NC",
                             "Non-edible", paste(Item_Fat_Content)),
    # Item_MRP cluster
    Item_MRP_cluster= factor(case_when(
      Item_MRP < Cutoff_Item_MRP[1] ~ 1,
      Item_MRP > Cutoff_Item_MRP[1] & Item_MRP < Cutoff_Item_MRP[2] ~ 2,
      Item_MRP > Cutoff_Item_MRP[2] & Item_MRP < Cutoff_Item_MRP[3] ~ 3,
      TRUE ~ 4
    ))
    
    )
})()

```

```{r Impute_NA_create_variables_pt2}
# Imputing missing values for Item_Weight using Random Forest
# The use of the Random Forest algorithm to input missing values for Item_Weight is because it provides a lower RMSE than a model of linear regression.

set.seed(44)
Weights_datasets <- (function(){
  
  #Extract remove NAs from Item_Weight, and column Item_Identifier
  BigMart_weight <- BigMart %>% 
    select(-c(Item_Outlet_Sales,
              Item_Identifier,
              #OUT019 & OUT027 only have itens without weight
              Outlet_Identifier,
              #Supermarket type3 also only have itens without weight
              Outlet_Type
    )) %>% 
    drop_na(Item_Weight)
   # select(-Item_Identifier) %>%

  
  #Create test and train datasets.
  

  Item_weight_index <- caret::createDataPartition(
    BigMart_weight$Item_Weight, times = 1, p=.8, list=F)
  
  Train_weight <- BigMart_weight[Item_weight_index,]
  Test_weight <- BigMart_weight[-Item_weight_index,]
  
  list(Train= Train_weight,
       Test= Test_weight)
  
})()


# RMSE from linear model
(function(){

  # Model
  lm_fit <- Weights_datasets$Train %>% 
    lm(Item_Weight ~ ., data=.)

  Pred_RMSE(lm_fit,  Weights_datasets$Test)
 
})() # RMSE= 4.64309


# Random Forest model
# Finding best mtry value
set.seed(44)
rf_weights_mtry <- randomForest::tuneRF(Weights_datasets$Train,
                     Weights_datasets$Train$Item_Weight,
                     stepFactor = 1,
                     improve= 1e-5,
                     ntree=161)





# Training the Forest

registerDoParallel(makeCluster(detectCores()))
set.seed(44)
rf_weight1 <- Weights_datasets$Train %>%
  train(Item_Weight~.,
        data=.,
        method="rf",
        ntree=150,
        tuneLength= rf_weights_mtry[1], #this is the mtry of randomForest package
        metric="RMSE",
        maximize=FALSE,
        trControl= trainControl(
          method="repeatedcv", #k-fold com repetições, method=cv é k-fold sem rep
          number= 10,
          repeats=3,
          savePredictions = TRUE,
          returnResamp = "all",
          allowParallel = TRUE))

                        
# RMSE from Random Forest
Pred_RMSE(rf_weight1, Weights_datasets$Test)
# RMSE= 2.2212.


# Graph: RF fitting
tibble(y=BigMart$Item_Weight,
       y_hat= predict(rf_weight1, BigMart)) %>%
  ggplot(aes(y_hat, y))+
  geom_point(color="#6669A9", alpha=.3)+
  labs(y="Observed values",
       x="Predicted values",
       title = "Random Forest fitting")



# Replace missing values for predicted values from the Random Forest model.
BigMart <- BigMart %>% mutate(
  Item_Weight= coalesce(Item_Weight,
                        predict(rf_weight1, BigMart)),
                        # Calculate item price per weight
                        Item_MRP_per_Weight= Item_MRP/Item_Weight)


  

```

```{r, Predict_Item_Outlet_Sales_LM_model}
#####Predicting Item_Outlet_Sales

# Update train and test set
BigMart_train <- BigMart %>% slice(nrow(BigMart_test)+1:nrow(BigMart_train))
BigMart_test <- BigMart %>% slice(1:nrow(BigMart_test))

#Separate train dataset into test and train
set.seed(44)
Index_train_sales <- createDataPartition(BigMart_train$Item_Outlet_Sales,
                                         times=1,
                                         p=.8, 
                                         list=FALSE)


Train_sales <- BigMart_train[Index_train_sales,] %>% select(-Item_Identifier)
Test_sales <- BigMart_train[-Index_train_sales,] %>% select(-Item_Identifier)



#####LM Model

(function(){
  
  ####Correcting skewness of Item_Visibility and Item_MRP_per_Weight
BigMart_lm <- BigMart_train %>% mutate(
  Item_Visibility= log(Item_Visibility+1), 
  Item_MRP_per_Weight= log(Item_MRP_per_Weight+1)
)

LM_train <- BigMart_lm[Index_train_sales,] %>% select(-Item_Identifier)
LM_test <- BigMart_lm[-Index_train_sales,] %>% select(-Item_Identifier)

LM_Sales_fit <- LM_train %>% lm(Item_Outlet_Sales ~., data=.)


Pred_RMSE(LM_Sales_fit, LM_test)
 

})() #RMSE= 1134.312






```

```{r Predict_Item_Outlet_Sales_RF_model}
# Best mtry
set.seed(44)
Sales_mtry <- randomForest::tuneRF(Train_sales,
                                  Train_sales$Item_Outlet_Sales,
                                  stepFactor = 1,
                                  improve= 1e-5,
                                  ntree=500)



cluster <- makeCluster(detectCores())
registerDoParallel(makeCluster(detectCores()))

# Training the Forest
set.seed(44)
system.time(rf_sales_fit <-
              Train_sales %>%
              train(Item_Outlet_Sales ~.,
                    data=.,
                    method="rf",
                    ntree=150,
                    tuneLength= Sales_mtry[1], #this is the mtry of randomForest package
                    metric="RMSE",
                    maximize=FALSE,
                    trControl= trainControl(
                      method="repeatedcv", #k-fold com repetições, method=cv é k-fold sem rep
                      number= 10,
                      repeats=3,
                      savePredictions = TRUE,
                      returnResamp = "all",
                      allowParallel = TRUE)))


# close cluster
stopCluster(cluster)

# RMSE from rf
Pred_RMSE(rf_sales_fit, Test_sales) # RMSE= 1118.098
```

```{r rf_fit2}

cluster <- makeCluster(detectCores())
registerDoParallel(makeCluster(detectCores()))

# Training the Forest
set.seed(44)
system.time(rf_sales_fit2 <-
              Train_sales %>%
              train(Item_Outlet_Sales ~.,
                    data=.,
                    method="ranger",
                    num.trees=500,
                    tuneGrid= expand.grid(
                      mtry=Sales_mtry[1],
                      splitrule="variance",
                      min.node.size=20), #Increased because of overfitting 
                    metric="RMSE",
                    maximize=FALSE,
                    trControl= trainControl(
                      method="repeatedcv", #k-fold com repetições, method=cv é k-fold sem rep
                      number= 10,
                      repeats=3,
                      savePredictions = TRUE,
                      returnResamp = "all",
                      allowParallel = TRUE)))


Pred_RMSE(rf_sales_fit2, Test_sales)

Submission_rf <- tibble(Item_Identifier= BigMart_test$Item_Identifier, 
                        Outlet_Identifier= BigMart_test$Outlet_Identifier,
                        Item_Outlet_Sales= predict(rf_sales_fit2, BigMart_test))

write.csv(Submission_rf, "rf_submit.csv", row.names=F)

```

```{r xgboost}

BigMartXGB <- (function(){
  #Select factor variables and make dummy
  Fac_BigMart <- BigMart %>% select(where(is.factor),
                                    -Item_Identifier)
  Dummy <- Fac_BigMart %>% dummyVars("~ . ", data=.)
  
  Dummy_df <- data.frame(predict(Dummy, newdata=Fac_BigMart))
  
  #Select Numeric variables and scale them
  Num_BigMart <- BigMart %>% select(where(is.numeric), -Item_Outlet_Sales)
  
  Num_BigMart_pP <- preProcess(Num_BigMart, methods= c("center", "scale"))
  
  Num_BigMart_scl <- predict(Num_BigMart_pP, Num_BigMart  )
  
  #Merge numeric variables with dummy ones
  Complete_data <- cbind(BigMart$Item_Outlet_Sales,
        Num_BigMart_scl,
        Dummy_df) %>% rename(Item_Outlet_Sales = 1)
  
  #Separate into the original train and test datasets
  Train <- Complete_data %>% slice(nrow(BigMart_test)+1:nrow(BigMart_train))
  Test <- Complete_data %>% slice(1:nrow(BigMart_test))
  
  list(Complete_data=Complete_data,
       Train=Train, 
       Test=Test)

})()


xgb_Train <- BigMartXGB$Train[Index_train_sales,]
xgb_Test <- BigMartXGB$Train[-Index_train_sales,]




xgb_fit_default <- xgb_Train %>% caret::train(Item_Outlet_Sales ~ .,
                                              data=.,
                                              method="xgbTree",
                                              verbose=TRUE, 
                                              metric="RMSE",
                                              maximize=FALSE)

Pred_RMSE(xgb_fit_default, xgb_Test) #1083.994


# This gives negative values
registerDoParallel(makeCluster(detectCores()))
set.seed(44)
#grid search xgb_fit
xgb_fit <- xgb_Train %>% caret::train(Item_Outlet_Sales ~ .,
                                      data=.,
                                      method="xgbTree",
                                      tree_method="hist", #To run faster
                                      verbose=TRUE, 
                                      metric="RMSE",
                                      maximize=FALSE,
                                      
                                      trControl= trainControl(
                                        method="repeatedcv",
                                        number=10,
                                        repeats=5,
                                        verboseIter = TRUE, #To have training log,
                                        allowParallel=T,
                                      ),
                                      tuneGrid= expand.grid(
                                        nrounds=200, #number of trees, default= 100
                                        max_depth= 2, #default =6
                                        eta= .04, #learning rate, default= .3
                                        gamma=0.75, #default=0
                                        colsample_bytree= .8, #default=1 
                                        min_child_weight= 58, #default=1
                                        subsample=1 #defaut=1
                                      ))


#On the full Train data
xgb_fit_full <- BigMartXGB$Train %>%
  caret::train(Item_Outlet_Sales ~ .,
               data=.,
               method="xgbTree",
               tree_method="hist", #To run faster
               verbose=TRUE, 
               metric="RMSE",
               maximize=FALSE,
               
               trControl= trainControl(
                 method="repeatedcv",
                 number=10,
                 repeats=5,
                 verboseIter = TRUE, #To have training log,
                 allowParallel=T,
               ),
               tuneGrid= expand.grid(
                 nrounds=200, #number of trees, default= 100
                 max_depth= 2, #default =6
                 eta= .04, #learning rate, default= .3
                 gamma=0.75, #default=0
                 colsample_bytree= .8, #default=1 
                 min_child_weight= 58, #default=1
                                        subsample=1 #defaut=1
                                      ))


Pred_RMSE(xgb_fit_full, xgb_Test) #RMSE=1080.803





registerDoParallel(makeCluster(detectCores()))
set.seed(44)
#grid search xgb_fit
xgb_fit_3 <- xgb_Train %>%
  caret::train(Item_Outlet_Sales ~ .,
               data=.,
               method="xgbTree",
               tree_method="hist", #To run faster
               objective="reg:tweedie", #To avoid neg values
               tweedie_variance_power=1.2, #Poisson distribution
               verbose=TRUE, 
               metric="RMSE",
               maximize=FALSE,
               set.seed(44),
               
               trControl= trainControl(
                 method="repeatedcv",
                 number=10,
                 repeats=5,
                 verboseIter = TRUE, #To have training log,
                 allowParallel=T,
               ),
               tuneGrid= expand.grid(
                 nrounds=seq(200, 1000, 200), #number of trees, default= 100
                 max_depth= 2, #default =6
                 eta= .04, #learning rate, default= .3
                 gamma=0.75, #default=0
                 colsample_bytree= .8, #default=1 
                 min_child_weight= 58, #default=1
                 subsample=1 #defaut=1
               ))

Pred_RMSE(xgb_fit_3, xgb_Test)

Submission <- tibble(Item_Identifier= BigMart_test$Item_Identifier, 
                        Outlet_Identifier= BigMart_test$Outlet_Identifier,
                        Item_Outlet_Sales= predict(xgb_fit_3, BigMartXGB$Test))

write.csv(Submission, "xgb_submit.csv", row.names=F)

```
